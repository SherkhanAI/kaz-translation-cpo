# Dataset for Translator - Requirements
# Python 3.11
#
# INSTALLATION ORDER:
# 1. pip3 install torch torchvision  (for H100/CUDA)
# 2. conda install -c pytorch -c nvidia faiss-gpu=1.9.0  (or pip install faiss-cpu)
# 3. pip install -r requirements.txt

# =============================================================================
# Phase 1: PDF Extraction
# =============================================================================
pypdf>=4.0.0            # Fast text extraction for digital PDFs
docling>=2.15.0         # OCR fallback for scanned PDFs
docling-core>=2.0.0

# =============================================================================
# Phase 2: Sentence Chunking
# =============================================================================
# Built-in regex is sufficient, no extra deps needed

# =============================================================================
# Phase 3: Embedding & Alignment
# =============================================================================
sonar-space>=0.3.0      # Meta's SONAR - multilingual embeddings (200+ languages)
sentence-transformers>=3.3.0  # MEXMA + LaBSE via SentenceTransformer API
bertalign @ git+https://github.com/bfsujason/bertalign.git  # m:n sentence alignment with DP
numba>=0.60.0           # Required by bertalign for JIT-compiled DP
sentence-splitter>=1.4  # Required by bertalign
# faiss-gpu - install via conda: conda install -c pytorch -c nvidia faiss-gpu=1.9.0
# OR for CPU: pip install faiss-cpu
transformers>=4.45.0

# =============================================================================
# Phase 4: CPO Training
# =============================================================================
trl>=0.12.0
peft>=0.13.0
accelerate>=1.0.0
bitsandbytes>=0.44.0
datasets>=3.0.0

# =============================================================================
# Utilities
# =============================================================================
numpy>=1.26.0
pandas>=2.2.0
tqdm>=4.66.0
jsonlines>=4.0.0
wandb>=0.18.0  # For experiment tracking

# =============================================================================
# Evaluation
# =============================================================================
sacrebleu>=2.4.0
comet-ml>=3.35.0
unbabel-comet>=2.2.0  # COMET metric for translation quality
